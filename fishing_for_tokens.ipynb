{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üé£ Fishing for Tokens: A Gentle Dive into How LLMs Work\n",
    "\n",
    "Just like fishing, working with a Large Language Model (LLM) is about casting your question into a vast sea of knowledge and reeling in the most likely *catch* ‚Äî the next word, phrase, or idea.\n",
    "\n",
    "This notebook walks through the main stages of how a language model (like GPT) turns your prompt into an intelligent-sounding response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Setup\n",
    "We'll use the **Hugging Face Transformers** library for a lightweight demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '.venv (Python 3.13.8)' requires the ipykernel package.\n",
      "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '\"c:/Misc/My Personal/Calipso/MyPersonal/Blog/LLM-Jupyter/.venv/Scripts/python.exe\" -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "%pip install transformers torch matplotlib numpy plotly -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Casting the Line ‚Äì Tokenization\n",
    "\n",
    "The tokenizer breaks text into tokens ‚Äî the atomic units the model understands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilgpt2')\n",
    "prompt = \"Please tell me how to describe an LLM model.\"\n",
    "\n",
    "tokens = tokenizer.tokenize(prompt)\n",
    "print('ü™£ Tokens:', tokens)\n",
    "\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print('\\n Token IDs:', token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each token is a fragment of text. GPT models don't process entire words but smaller subword pieces ‚Äî like catching fish of different sizes from the same pond."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Dropping the Bait ‚Äì Embeddings\n",
    "\n",
    "Each token is mapped to a **vector** ‚Äî a numerical representation of meaning and context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors='pt')\n",
    "print('üéØ Input IDs:', inputs['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These numbers correspond to positions in a learned embedding space ‚Äî where similar meanings (like *fish* and *salmon*) are close together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Sensing the Current ‚Äì Transformer Attention\n",
    "\n",
    "Attention allows the model to decide which tokens matter most to each other.\n",
    "\n",
    "Let's visualize one of the attention heads to see how relationships form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model = AutoModel.from_pretrained('distilgpt2', output_attentions=True)\n",
    "outputs = model(**inputs)\n",
    "attn = outputs.attentions[-1][0].detach().numpy()\n",
    "\n",
    "plt.imshow(attn.mean(axis=0), cmap='Blues')\n",
    "plt.title('üé£ Average Attention Map')\n",
    "plt.xlabel('Key Tokens')\n",
    "plt.ylabel('Query Tokens')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this heatmap, darker areas show stronger relationships ‚Äî the model pays more attention to those tokens when generating the next word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Processing the Catch ‚Äì Feed Forward & Normalization\n",
    "\n",
    "Each token‚Äôs attention-weighted view passes through dense layers (feed-forward networks) and normalization.\n",
    "\n",
    "This helps the model keep the meaning stable while refining context ‚Äî like filtering and sorting your catch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Average hidden state value:', outputs.last_hidden_state.mean().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Reeling It In ‚Äì Token Generation\n",
    "\n",
    "Now let's simulate what happens when the model generates text ‚Äî predicting one token at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "generator = AutoModelForCausalLM.from_pretrained('distilgpt2')\n",
    "output_ids = generator.generate(**inputs, max_length=40, temperature=0.7)\n",
    "\n",
    "response = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "print('ü§ñ Model Response:\\n', response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each token generated is like another *fish reeled in* ‚Äî probabilistic, context-aware, and slightly unpredictable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Delivering the Fish ‚Äì Simulated API Response\n",
    "\n",
    "Let's represent what a typical API call might return to an application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "api_response = {\n",
    "    'model': 'distilgpt2',\n",
    "    'user_id': 'user123',\n",
    "    'prompt': prompt,\n",
    "    'response': response\n",
    "}\n",
    "\n",
    "print(json.dumps(api_response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Building a Better Boat ‚Äì Optimization\n",
    "\n",
    "The process is far from simple ‚Äî much like any ecosystem. Unlike the efficiency of nature, LLMs require enormous computational resources and expensive datasets.\n",
    "\n",
    "Researchers are working to make these systems leaner and faster through:\n",
    "- **Parameter pruning** (removing unnecessary weights)\n",
    "- **Quantization** (using fewer bits per parameter)\n",
    "- **Model distillation** (teaching smaller models from big ones)\n",
    "- **FlashAttention & LoRA** (faster, more memory-efficient training)\n",
    "\n",
    "Each step is like upgrading your fishing gear ‚Äî faster, lighter, more sustainable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß© Summary\n",
    "\n",
    "- You cast a prompt (tokenization)\n",
    "- The model drops bait (embeddings)\n",
    "- Senses the current (attention)\n",
    "- Filters the catch (feed-forward layers)\n",
    "- Reels in tokens (generation)\n",
    "- Sends them to your app (API response)\n",
    "\n",
    "In the end, AI fishing is all about patience, precision, and probabilities üé£"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
